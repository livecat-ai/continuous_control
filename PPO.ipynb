{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.36 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mjupyter-console 6.4.3 has requirement jupyter-client>=7.0.0, but you'll have jupyter-client 5.2.4 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from workspace_utils import keep_awake\n",
    "import jdc\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "# env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import MultivariateNormal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the mlp network.\n",
    "We will use this for both the actor and critic networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard in_dim-64-64-out_dim Feed Forward Neural Network.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, seed=42):\n",
    "        \"\"\"\n",
    "        Initialize the network and set up the layers.\n",
    "\n",
    "            Parameters:\n",
    "                in_dim - input dimensions as an int\n",
    "                out_dim - output dimensions as an int\n",
    "\n",
    "            Return:\n",
    "                None\n",
    "        \"\"\"\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.layer1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        \"\"\"\n",
    "            Runs a forward pass on the neural network.\n",
    "\n",
    "            Parameters:\n",
    "                obs - observation to pass as input\n",
    "\n",
    "            Return:\n",
    "                output - the output of our forward pass\n",
    "        \"\"\"\n",
    "        # Convert observation to tensor if it's a numpy array\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "        x = F.relu(self.layer1(obs))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        output = self.layer3(x)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check which device we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using: \", device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement our PPO agent class.\n",
    "Here we setup the environment, our actor and critic models plus other required variables.\n",
    "We are using the jdc module to make it easer to work with classes in jupyter notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, env, hidden_dim, \n",
    "                 lr = 1e-3, \n",
    "                 clip_epsilon=0.1, \n",
    "                 discount=0.95, \n",
    "                 n_updates = 5\n",
    "                 ):\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_updates = n_updates\n",
    "        self.lr = lr\n",
    "        self.discount = discount\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        # Environment variables\n",
    "        self.env = env\n",
    "        self.brain_name = env.brain_names[0]\n",
    "        self.brain = env.brains[self.brain_name]\n",
    "        self.action_size = self.brain.vector_action_space_size\n",
    "        self.state_size = self.brain.vector_observation_space_size\n",
    "\n",
    "        self.actor = FeedForwardNN(self.state_size, self.hidden_dim, self.action_size).to(device)                                                   # ALG STEP 1\n",
    "        self.critic = FeedForwardNN(self.state_size, self.hidden_dim, 1).to(device)\n",
    "\n",
    "        # Initialize optimizers for actor and critic\n",
    "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=self.lr)\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=self.lr)\n",
    "\n",
    "        # Initialize the covariance matrix used to query the actor for actions\n",
    "        self.cov_var = torch.full(size=(self.action_size,), fill_value=0.5)\n",
    "        self.cov_mat = torch.diag(self.cov_var)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_actions\n",
    "This method takes a batch of states from the environment and returns an action and the log_probs of the action.\n",
    "We do this using torch.MultivariateNormal to sample an action rather than taking the action directly from the output of the actor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%add_to PPO\n",
    "\n",
    "def get_actions(self, states):\n",
    "    \"\"\"\n",
    "        Queries an action from the actor network, should be called from rollout.\n",
    "\n",
    "        Parameters:\n",
    "            obs - the observation at the current timestep\n",
    "\n",
    "        Return:\n",
    "            action - the action to take, as a numpy array\n",
    "            log_prob - the log probability of the selected action in the distribution\n",
    "    \"\"\"\n",
    "    # Query the actor network for a mean action\n",
    "    mean = self.actor(states).to(torch.device('cpu'))\n",
    "\n",
    "    dist = MultivariateNormal(mean, self.cov_mat)\n",
    "\n",
    "    # Sample an action from the distribution\n",
    "    action = dist.sample()\n",
    "\n",
    "\n",
    "    # Calculate the log probability for that action\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    return action, log_prob.detach()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute_actor_log_probs\n",
    "Given a batch of states and actions it returns the log_probs of the input actions given the current current action model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%add_to PPO\n",
    "\n",
    "def compute_actor_log_probs(self, batch_obs, batch_acts):\n",
    "    \"\"\"\n",
    "        Estimate the log probs of\n",
    "        each action in the most recent batch with the most recent\n",
    "        iteration of the actor network. Should be called from learn.\n",
    "\n",
    "        Parameters:\n",
    "            batch_obs - the observations from the most recently collected batch as a tensor.\n",
    "                        Shape: (number of timesteps in batch, dimension of observation)\n",
    "            batch_acts - the actions from the most recently collected batch as a tensor.\n",
    "                        Shape: (number of timesteps in batch, dimension of action)\n",
    "\n",
    "        Return:\n",
    "            log_probs - the log probabilities of the actions taken in batch_acts given batch_obs\n",
    "    \"\"\"\n",
    "    # Calculate the log probabilities of batch actions using most recent actor network.\n",
    "    # This segment of code is similar to that in get_action()\n",
    "    mean = self.actor(batch_obs).to(torch.device('cpu'))\n",
    "    dist = MultivariateNormal(mean, self.cov_mat)\n",
    "    log_probs = dist.log_prob(batch_acts)\n",
    "\n",
    "    # Return the value vector V of each observation in the batch\n",
    "    # and log probabilities log_probs of each action in the batch\n",
    "    return log_probs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods to compute the normalized forward returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%add_to PPO\n",
    "\n",
    "def compute_future_rewards(self, rewards):\n",
    "        discounts = self.discount**np.arange(len(rewards))\n",
    "        discounted_rewards = np.asarray(rewards) * discounts[:,np.newaxis]\n",
    "        future_rewards = discounted_rewards[::-1].cumsum(axis=0)[::-1]\n",
    "        return future_rewards\n",
    "\n",
    "def compute_normalize_future_rewards(self, rewards):\n",
    "    future_rewards = self.compute_future_rewards(rewards)\n",
    "    mean = np.mean(future_rewards, axis=1)\n",
    "    std = np.std(future_rewards, axis=1) + 1.0e-10\n",
    "    normalized_rewards = (future_rewards - mean[:, np.newaxis]) / std[:, np.newaxis]\n",
    "    return torch.tensor(normalized_rewards, dtype=torch.float32).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolls out full epsiodes for all the agents.\n",
    "In this case 20 agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%add_to PPO\n",
    "\n",
    "def rollout(self, steps):\n",
    "    env_info = self.env.reset(train_mode=True)[self.brain_name]\n",
    "    num_agents = len(env_info.agents)\n",
    "    state = env_info.vector_observations \n",
    "    \n",
    "    state_list = []\n",
    "    action_list = []\n",
    "    reward_list = []\n",
    "    log_prob_list = []\n",
    "    done_list = []\n",
    "    \n",
    "    for i in range(steps):\n",
    "        state_tensor = torch.tensor(state, dtype = torch.float32).to(device)\n",
    "        action_tensor, prob_tensor = self.get_actions(state_tensor) \n",
    "        actions = np.clip(action_tensor.detach().numpy(), -1, 1)\n",
    "        \n",
    "        env_info = self.env.step(actions)[self.brain_name] \n",
    "        next_state = env_info.vector_observations         # get next state (for each agent)\n",
    "        reward = env_info.rewards                         # get reward (for each agent)\n",
    "        done = env_info.local_done\n",
    "        \n",
    "        state_list.append(state_tensor)\n",
    "        action_list.append(action_tensor)\n",
    "        log_prob_list.append(prob_tensor)\n",
    "        reward_list.append(reward)\n",
    "        done_list.append(done)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if np.any(done):\n",
    "            break\n",
    "        \n",
    "    states_tensor = torch.stack(state_list).to(device)\n",
    "    actions_tensor = torch.stack(action_list)\n",
    "    log_probs_tensor = torch.stack(log_prob_list)\n",
    "    rewards_tensor = torch.tensor(reward_list, dtype=torch.float32)\n",
    "    dones_tensor = torch.tensor(done_list, dtype=torch.float32)\n",
    "    \n",
    "    return (states_tensor, actions_tensor, log_probs_tensor, rewards_tensor, dones_tensor)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to compute the advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%add_to PPO\n",
    "\n",
    "def compute_advantage(self, states, forward_normalised_returns):\n",
    "    \"\"\"\n",
    "        Estimate the values of each observation of\n",
    "        each action in the most recent batch with the most recent\n",
    "        iteration of the actor network. Should be called from learn.\n",
    "    \"\"\"\n",
    "    V = self.critic(states).squeeze()\n",
    "    adv = forward_normalised_returns - V.detach()                                                                      # ALG STEP 5\n",
    "    adv = (adv - adv.mean()) / (adv.std() + 1e-10)\n",
    "    return adv.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This method runs the learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%add_to PPO\n",
    "def learn(self, num_episodes, num_agents=20, print_every=10, max_episode_steps=1000):\n",
    "    # Create a batch of environments\n",
    "    losses = []\n",
    "    rewards = []\n",
    "    scores = np.zeros(num_agents)\n",
    "    for episode in keep_awake(range(1, num_episodes + 1)):\n",
    "        # Step all the environments\n",
    "        states_tensor, actions_tensor, log_probs_tensor, rewards_tensor, dones_tensor = self.rollout(max_episode_steps)\n",
    "    \n",
    "        rewards_normalize_future = self.compute_normalize_future_rewards(rewards_tensor)\n",
    "        \n",
    "        # Calculate advantage at k-th iteration\n",
    "        A_k = self.compute_advantage(states_tensor, rewards_normalize_future)\n",
    "        \n",
    "        for _ in range(self.n_updates): \n",
    "\n",
    "            V = self.critic(states_tensor).squeeze()\n",
    "          \n",
    "            curr_log_probs = self.compute_actor_log_probs(states_tensor, actions_tensor)\n",
    "            \n",
    "            ratios = torch.exp(curr_log_probs - log_probs_tensor).to(device)\n",
    "            \n",
    "            surr1 = ratios * A_k\n",
    "\n",
    "            surr2 = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * A_k\n",
    "            \n",
    "            actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "            \n",
    "            # Calculate actor and critic losses.\n",
    "            critic_loss = nn.MSELoss()(V, rewards_normalize_future)\n",
    "\n",
    "            # Calculate gradients and perform backward propagation for actor network\n",
    "            self.actor_optim.zero_grad()\n",
    "            actor_loss.backward(retain_graph=True)\n",
    "            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1)\n",
    "            self.actor_optim.step()\n",
    "\n",
    "            # Calculate gradients and perform backward propagation for critic network\n",
    "            self.critic_optim.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 1)\n",
    "            self.critic_optim.step()\n",
    "    \n",
    "        cum_rewards = rewards_tensor.numpy().sum(axis=0)\n",
    "        rewards.append(np.mean(cum_rewards))\n",
    "        scores += cum_rewards\n",
    "           \n",
    "        if episode % print_every == 0:\n",
    "            print('Ep {}: Total score (averaged over agents) this episode: {}'.format(episode, np.mean(scores) / print_every))\n",
    "            scores = np.zeros(num_agents)\n",
    "            torch.save(self.actor.state_dict(), './ppo_actor.pth')\n",
    "            torch.save(self.critic.state_dict(), './ppo_critic.pth')\n",
    "    return rewards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "num_updates = 5\n",
    "learning_rate = 5e-3\n",
    "\n",
    "ppo_agent = PPO(env, hidden_size,\n",
    "                lr = learning_rate,\n",
    "                n_updates=num_updates)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 10: Total score (averaged over agents) this episode: 0.7725499014183879\n",
      "Ep 20: Total score (averaged over agents) this episode: 1.1824997296743096\n",
      "Ep 30: Total score (averaged over agents) this episode: 1.7212494010478259\n",
      "Ep 40: Total score (averaged over agents) this episode: 2.187749004624784\n",
      "Ep 50: Total score (averaged over agents) this episode: 2.751298558115959\n",
      "Ep 60: Total score (averaged over agents) this episode: 3.6034979252517223\n",
      "Ep 70: Total score (averaged over agents) this episode: 4.11189755871892\n",
      "Ep 80: Total score (averaged over agents) this episode: 4.7701971811428665\n",
      "Ep 90: Total score (averaged over agents) this episode: 5.537746739014983\n",
      "Ep 100: Total score (averaged over agents) this episode: 6.150946439281106\n",
      "Ep 110: Total score (averaged over agents) this episode: 7.311345716714859\n",
      "Ep 120: Total score (averaged over agents) this episode: 8.162445883750916\n",
      "Ep 130: Total score (averaged over agents) this episode: 9.319145819991828\n",
      "Ep 140: Total score (averaged over agents) this episode: 10.542693819180132\n",
      "Ep 150: Total score (averaged over agents) this episode: 12.317048988342284\n",
      "Ep 160: Total score (averaged over agents) this episode: 14.112459369897843\n",
      "Ep 170: Total score (averaged over agents) this episode: 15.300371054410935\n",
      "Ep 180: Total score (averaged over agents) this episode: 15.538772205114364\n",
      "Ep 190: Total score (averaged over agents) this episode: 17.703144369125365\n",
      "Ep 200: Total score (averaged over agents) this episode: 19.966685448884967\n",
      "Ep 210: Total score (averaged over agents) this episode: 20.72220067501068\n",
      "Ep 220: Total score (averaged over agents) this episode: 21.445470513403414\n",
      "Ep 230: Total score (averaged over agents) this episode: 22.18463281482458\n",
      "Ep 240: Total score (averaged over agents) this episode: 23.141405133008956\n",
      "Ep 250: Total score (averaged over agents) this episode: 23.19600304365158\n",
      "Ep 260: Total score (averaged over agents) this episode: 23.7542663192749\n",
      "Ep 270: Total score (averaged over agents) this episode: 25.144846711158753\n",
      "Ep 280: Total score (averaged over agents) this episode: 25.062298844307662\n",
      "Ep 290: Total score (averaged over agents) this episode: 26.034818410873413\n",
      "Ep 300: Total score (averaged over agents) this episode: 26.150221347808838\n",
      "Ep 310: Total score (averaged over agents) this episode: 26.55472950935364\n",
      "Ep 320: Total score (averaged over agents) this episode: 26.286324467658993\n",
      "Ep 330: Total score (averaged over agents) this episode: 27.01143973350525\n",
      "Ep 340: Total score (averaged over agents) this episode: 27.37340085029602\n",
      "Ep 350: Total score (averaged over agents) this episode: 27.7369605743885\n",
      "Ep 360: Total score (averaged over agents) this episode: 27.971312267780302\n",
      "Ep 370: Total score (averaged over agents) this episode: 28.226767296791074\n",
      "Ep 380: Total score (averaged over agents) this episode: 28.110664658546447\n",
      "Ep 390: Total score (averaged over agents) this episode: 28.736678662300108\n",
      "Ep 400: Total score (averaged over agents) this episode: 29.158238620758056\n",
      "Ep 410: Total score (averaged over agents) this episode: 29.54009817838669\n",
      "Ep 420: Total score (averaged over agents) this episode: 28.93308506011963\n",
      "Ep 430: Total score (averaged over agents) this episode: 29.433794355392457\n",
      "Ep 440: Total score (averaged over agents) this episode: 29.4702965259552\n",
      "Ep 450: Total score (averaged over agents) this episode: 29.714051275253297\n",
      "Ep 460: Total score (averaged over agents) this episode: 30.805375280380247\n",
      "Ep 470: Total score (averaged over agents) this episode: 31.337037191390994\n",
      "Ep 480: Total score (averaged over agents) this episode: 31.57989447593689\n",
      "Ep 490: Total score (averaged over agents) this episode: 31.98120170593262\n",
      "Ep 500: Total score (averaged over agents) this episode: 33.228429822921754\n",
      "Ep 510: Total score (averaged over agents) this episode: 33.90229563236237\n",
      "Ep 520: Total score (averaged over agents) this episode: 34.534909720420835\n",
      "Ep 530: Total score (averaged over agents) this episode: 35.90764096736908\n",
      "Ep 540: Total score (averaged over agents) this episode: 36.69645851135254\n",
      "Ep 550: Total score (averaged over agents) this episode: 37.622830104827884\n",
      "Ep 560: Total score (averaged over agents) this episode: 38.29074514389038\n",
      "Ep 570: Total score (averaged over agents) this episode: 38.70495454788208\n",
      "Ep 580: Total score (averaged over agents) this episode: 39.051062908172604\n",
      "Ep 590: Total score (averaged over agents) this episode: 39.116014366149905\n",
      "Ep 600: Total score (averaged over agents) this episode: 39.17181571960449\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 600\n",
    "max_steps = 1000\n",
    "scores = ppo_agent.learn(num_episodes, max_episode_steps=max_steps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display a learning graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f16cd552a58>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
